{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc9d331",
   "metadata": {},
   "source": [
    "#### <h1>Machine Learning</h1>\n",
    "\n",
    "<h2>Lab assignment 5: Spam filter</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93de75-0f55-4a10-965e-c9cc62689abc",
   "metadata": {},
   "source": [
    "Current email services provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In the following you experiment known classifiers to build your own spam filter.\n",
    "\n",
    "The goal is to discriminate whether a given email, $x$, is spam ($y$=1) or non-spam ($y$=0). For this you need to convert each email into a feature vector $\\vec{x} \\in \\{0, 1\\}^n$. The following will walk you through how such a feature vector can be constructed from an email.\n",
    "\n",
    "Throughout the rest of this lab, you will be using the datasets included which are a subset of the Spam Assassin Public Corpus[1]. For the purpose of this lab, you will only be using the body of the email (excluding the email headers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c72c83",
   "metadata": {},
   "source": [
    "![sampleEmail](sampleEmail.png \"sampleEmail\") \\\n",
    "Fig 1. Sample email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887bdae-48e9-4ea3-88e5-e02ed436c236",
   "metadata": {},
   "source": [
    "\\\n",
    "Before starting a machine learning task, it is usually useful to take a look at examples from the dataset. For this we need to import some packages and a write a little readFile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6a3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "import csv\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60ef9",
   "metadata": {},
   "source": [
    "Now we read the text file with the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad4ae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email\n",
      "> Anyone knows how much it costs to host a web portal ?>Well, it depends on how many visitors you're expecting.This can be anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..To unsubscribe yourself from this mailing list, send an email to:groupname-unsubscribe@egroups.com\n"
     ]
    }
   ],
   "source": [
    "def readFile(filename = None):\n",
    "    #READFILE reads a file and returns its entire contents\n",
    "    #   file_contents = READFILE(filename) reads a file and returns its entire\n",
    "    #   contents in file_contents\n",
    "    with open(filename, 'r') as file:\n",
    "        file_contents = file.read().replace('\\n', '')\n",
    "    return file_contents\n",
    "\n",
    "\n",
    "file_contents = readFile('emailSample1.txt')\n",
    "\n",
    "print('Original email')\n",
    "print(file_contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecb8f8",
   "metadata": {},
   "source": [
    "**Preprocessing Emails**\n",
    "\n",
    "The above is a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string “httpaddr\" to indicate that a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "\n",
    "In the function processEmail, we have implemented the following email preprocessing and normalization steps:\n",
    "\n",
    "* Lower-casing: The entire email is converted into lower case, so that capitalization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "* Stripping HTML: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "* Normalizing URLs: All URLs are replaced with the text “httpaddr\". \n",
    "* Normalizing Email Addresses: All email addresses are replaced with the text “emailaddr\".\n",
    "* Normalizing Numbers: All numbers are replaced with the text “number\".\n",
    "* Normalizing Dollars: All dollar signs ($) are replaced with the text “dollar\".\n",
    "* Word Stemming: Words are reduced to their stemmed form. For example, “discount\", “discounts\", “discounted\" and “discounting\" are all replaced with “discount\". Sometimes, the Stemmer actually strips off additional characters from the end, so “include\", “includes\", “included\", and “including\" are all replaced with “includ\".\n",
    "* Removal of non-words: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22909b22",
   "metadata": {},
   "source": [
    "**Vocabulary List**\n",
    "\n",
    "After preprocessing the emails, we have a list of word for each email. The next step is to choose which words we would like to use in our classifier and which we would want to leave out.\n",
    "\n",
    "For this lab assignment, we have chosen only the most frequently occurring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to overfit our training set. The complete vocabulary list is in the file vocab.txt.\n",
    "\n",
    "Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. In practice, a vocabulary list with about 10,000 to 50,000 words is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36752c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Vocabulary\n",
    "\n",
    "def getVocabList():\n",
    "    vocabList = [' ' for i in range(1899)]\n",
    "    with open('vocab.txt') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            vocabList[line_count] = row[1]\n",
    "            line_count += 1\n",
    "    return vocabList\n",
    "\n",
    "vocabList = getVocabList()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c57f9a-0c2d-4c06-8a4a-eb7d7376d16c",
   "metadata": {},
   "source": [
    "---\n",
    "# Quiz\n",
    "\\\n",
    "From this point on, you will be asked to complete the code of some functions.\n",
    "\n",
    "The instructions to do so, are defined as comments, in the place where the code should be inserted, started by the word:\n",
    "\n",
    "**# Instructions:**\n",
    "\n",
    "The code must be inserted below the instructions, after the comment line:\n",
    "\n",
    "&#35; ====================== YOUR CODE HERE ====================== \n",
    "\n",
    "\\\n",
    "Take a look at the example below, function `processEmail()`. This function converts the email text into stemmed words, and then into a vector of vocables indexes, named **word_indices**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2bf7b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon know how much it cost to host a web portal well it depend on how mani \n",
      "visitor you re expect thi can be anywher from less than number buck a month \n",
      "to a coupl of dollar number you should checkout httpaddr or perhap amazon ec \n",
      "number if your run someth big to unsubscrib yourself from thi mail list send \n",
      "an email to emailaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "Word Indices:  [  86.  916.  794. 1077.  883.  370. 1699.  790. 1822. 1831.  883.  431.\n",
      " 1171.  794. 1002. 1893. 1364.  592. 1676.  238.  162.   89.  688.  945.\n",
      " 1663. 1120. 1062. 1699.  375. 1162.  477. 1120. 1893. 1510.  799. 1182.\n",
      " 1237.  512. 1120.  810. 1895. 1440. 1547.  181. 1699. 1758. 1896.  688.\n",
      " 1676.  992.  961. 1477.   71.  530. 1699.  531.]\n"
     ]
    }
   ],
   "source": [
    "def processEmail(email_contents = None):\n",
    "    #   word_indices = PROCESSEMAIL(email_contents) preprocesses\n",
    "    #   the body of an email and returns a list of indices of the\n",
    "    #   words contained in the email.\n",
    "\n",
    "    # ========================== Preprocess Email ===========================\n",
    "    # Headers\n",
    "    # Handle them bellow  if you are working with raw emails with the\n",
    "    # full headers\n",
    "\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "\n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # it with a space\n",
    "    pattern = '<[^<]+?>'\n",
    "    email_contents = re.sub(pattern, ' ', email_contents)\n",
    "\n",
    "    # Look for one or more characters between 0-9\n",
    "    pattern = r'[0-9]+'\n",
    "    # Match all digits in the string and replace them with 'number'\n",
    "    email_contents = re.sub(pattern, ' number', email_contents)\n",
    "\n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https://\n",
    "    pattern=r'(http|https)\\S+'\n",
    "    email_contents = re.sub(pattern, 'httpaddr', email_contents)\n",
    "\n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle\n",
    "    pattern = r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+'\n",
    "    email_contents = re.sub(pattern, 'emailaddr', email_contents)\n",
    "\n",
    "    pattern = r'\\$'\n",
    "    email_contents = re.sub(pattern, 'dollar', email_contents)\n",
    "\n",
    "    # ========================== Tokenize Email ===========================\n",
    "\n",
    "    # Output the email to screen as well\n",
    "    print('\\n==== Processed Email ====\\n\\n' % ())\n",
    "    # Process file\n",
    "    l = 0\n",
    "    \n",
    "    # Init return value\n",
    "    word_indices = np.array([])\n",
    "    for s in re.split(\"[ .:;\\\\-,']\",email_contents):\n",
    "        # Tokenize and also get rid of any punctuation\n",
    "        s = re.sub(r'[^\\w\\s]','', s)\n",
    "        # Remove any non alphanumeric characters\n",
    "        s = re.sub('[^0-9a-zA-Z]+', ' ', s)\n",
    "\n",
    "        # Stem the word\n",
    "        ps = nltk.stem.PorterStemmer()\n",
    "        s=ps.stem(s)\n",
    "\n",
    "        # Skip the word if it is too short\n",
    "        if len(s) < 1:\n",
    "            continue\n",
    "        # Look up the word in the dictionary and add to word_indices if\n",
    "        # found\n",
    "        \n",
    "        # ====================== YOUR CODE HERE ======================\n",
    "        # Instructions: Fill in this function to add the index of str to\n",
    "        #               word_indices if it is in the vocabulary. At this point\n",
    "        #               of the code, you have a stemmed word from the email in\n",
    "        #               the variable str. You should look up str in the\n",
    "        #               vocabulary list (vocabList). If a match exists, you\n",
    "        #               should add the index of the word to the word_indices\n",
    "        #               vector. Concretely, if str = 'action', then you should\n",
    "        #               look up the vocabulary list to find where in vocabList\n",
    "        #               'action' appears. For example, if vocabList{18} =\n",
    "        #               'action', then, you should add 18 to the word_indices\n",
    "        #               vector.\n",
    "       \n",
    "        for i in np.arange(0, len(vocabList)) :\n",
    "            if (vocabList[i] == s):\n",
    "                word_indices = np.append(word_indices, i+1)\n",
    "\n",
    "        # =============================================================\n",
    "        # Print to screen, ensuring that the output lines are not too long\n",
    "        if (l + len(s) + 1) > 78:\n",
    "            print()\n",
    "            l = 0\n",
    "        print('%s ' % (s),end='')\n",
    "        l = l + len(s) + 1\n",
    "        \n",
    "    # Print footer\n",
    "    print('\\n\\n=========================\\n' % ())\n",
    "    return word_indices\n",
    "\n",
    "word_indices = processEmail(file_contents)\n",
    "# Print Stats\n",
    "print('Word Indices: ', word_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ccac9",
   "metadata": {},
   "source": [
    "The above is the processed sample email. While preprocessing has left word fragments and non-words, this form turns out to be much easier to work with for performing feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a227c13",
   "metadata": {},
   "source": [
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list. For example, in the sample email, the word “anyone\" was first normalized to “anyon\" and then mapped onto the index 86 in the vocabulary list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c608d8d",
   "metadata": {},
   "source": [
    "**Your first task**\n",
    "\n",
    "Your first task is to complete the code in emailFeatures(word_indices = None) below that takes in a word_indices array and produces a feature vector from the word indices. In other words, you will now implement the feature extraction that converts each email into a vector in $\\{0, 1\\}^n$. For this exercise, you will be using $n$ = number of words in vocabulary list. Specifically, the feature $x_i \\in \\{0,1 \\}$ for an email corresponds to whether the $i$-th word in the dictionary occurs in the email. That is, $x_i=1$ if the $i$-th word is in the email and $x_i=0$ if the $i$-th word is not present in the email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "978cd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices = None):\n",
    "    #   x = EMAILFEATURES(word_indices) takes in a word_indices vector and\n",
    "    #   produces a feature vector from the word indices.\n",
    "\n",
    "    # Total number of words in the dictionary\n",
    "    n = len(vocabList)\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = np.zeros(n)\n",
    "  \n",
    "\n",
    "    # Instructions: Fill in this function to return a feature vector for the\n",
    "    #               given email (word_indices). To help make it easier to\n",
    "    #               process the emails, we have have already pre-processed each\n",
    "    #               email and converted each word in the email into an index in\n",
    "    #               a fixed dictionary (of 1899 words). The variable\n",
    "    #               word_indices contains the list of indices of the words\n",
    "    #               which occur in one email.\n",
    "\n",
    "    #               Concretely, if an email has the text:\n",
    "    #                  The quick brown fox jumped over the lazy dog.\n",
    "    #               Then, the word_indices vector for this text might look\n",
    "    #               like:\n",
    "    #                   60  100   33   44   10     53  60  58   5\n",
    "    #               where, we have mapped each word onto a number, for example:\n",
    "    #                   the   -- 60\n",
    "    #                   quick -- 100\n",
    "    #                   ...\n",
    "    #              (note: the above numbers are just an example and are not the\n",
    "    #               actual mappings).\n",
    "\n",
    "    #              Your task is take one such word_indices vector and construct\n",
    "    #              a binary feature vector that indicates whether a particular\n",
    "    #              word occurs in the email. That is, x(i) = 1 when word i\n",
    "    #              is present in the email. Concretely, if the word 'the' (say,\n",
    "    #              index 60) appears in the email, then x(60) = 1. The feature\n",
    "    #              vector should look like:\n",
    "\n",
    "    #              x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..];\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    for word_index in word_indices:\n",
    "        x[int(word_index)] = 1.0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "678d352c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from sample email (emailSample1.txt)\n",
      "\n",
      "Length of feature vector: 1899\n",
      "Number of non-zero entries: 46 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nExtracting features from sample email (emailSample1.txt)\\n')\n",
    "features = emailFeatures(word_indices)\n",
    "# Print Stats\n",
    "print('Length of feature vector: %d' % len(features))\n",
    "print('Number of non-zero entries: %d \\n' % sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbc163",
   "metadata": {},
   "source": [
    "**Second task**\n",
    "\n",
    "After you have completed the feature extraction functions, the next step will load a preprocessed training dataset that will be used to train a classifier. **spamTrain.mat** contains 4000 training examples of spam and non-spam email, while **spamTest.mat** contains 1000 test examples. Each original email was processed using the processEmail and emailFeatures functions and converted into a vector $x \\in \\{0, 1\\}^{1899}$. After loading the dataset, train a classifier of your choice for discriminating between spam ($y=1$) and non-spam ($y=0$) emails. \n",
    "\n",
    "Your current task is to train this classifier and record the achieved training accuracies in both the training and the test sets. It is recommended to regularize your classifier. You can use either your own developed classifier code or sklearn classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "817f9bc4-2c6a-43b4-87a8-cae6c792c8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import *\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "class IModel(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(X : NDArray, y : NDArray) -> None:\n",
    "        '''\n",
    "        X is a 2D array with n features accross columns and m data points across rows.\n",
    "        y is a column vector with m labels, one for each data point in X.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(xp : NDArray) -> NDArray:\n",
    "        '''\n",
    "        xp is a column vector with k data points, each with n features.\n",
    "        returns a vector with k predicted labels, one for each data point in vector xp.\n",
    "        '''\n",
    "        pass\n",
    "\n",
    "    \n",
    "    @abstractmethod\n",
    "    def theta() -> [None | NDArray]:\n",
    "        '''\n",
    "        returns the vector theta with k coefficients after model trained.\n",
    "        before training returns None\n",
    "        '''\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffaf053-2a44-4a10-b816-98f9e8b5a722",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor (IModel):\n",
    "    '''\n",
    "    Implements the Normal equation model without regularization\n",
    "    '''\n",
    "\n",
    "    #Constructor\n",
    "    def __init__(self) -> None:\n",
    "        self._theta = None\n",
    "\n",
    "    # mean square error\n",
    "    def mse(self, X_test: NDArray, y_test: NDArray) -> float:\n",
    "        m = X_test.shape[0]\n",
    "        h = self.predict(X_test)\n",
    "        r = h - y_test\n",
    "\n",
    "        return ((r.T @ r) / m)[0][0]\n",
    "\n",
    "    #predict after training\n",
    "    def predict(self, xp : NDArray) -> NDArray:\n",
    "        if self._theta is None:\n",
    "            raise Exception('It is needed to fit model first')\n",
    "        else:    \n",
    "            # add a column of 1s \n",
    "            xp = self._addOnesLeft(xp)\n",
    "\n",
    "            # prediction\n",
    "            return xp.dot(self._theta)\n",
    "\n",
    "    \n",
    "    # return values for theta found after training or None\n",
    "    def theta(self) -> [None | NDArray]:\n",
    "        return self._theta\n",
    "\n",
    "    \n",
    "    # add a column of 1s at the left\n",
    "    def _addOnesLeft(self, X:NDArray) -> NDArray:\n",
    "        return np.column_stack((np.ones_like(X[:,0]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ed2192a-85e2-461c-8b4c-2ca580ee6336",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegularizedModel (Predictor):\n",
    "    '''\n",
    "    Represents a model that is regularized with a lambda regularization hperparameter\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, reg_lambda: float) -> None:\n",
    "        self.set_lambda(reg_lambda)\n",
    "        super().__init__()\n",
    "\n",
    "    \n",
    "    def find_optimal_lambda(self, X_train: NDArray, y_train: NDArray, X_test: NDArray, y_test: NDArray,\n",
    "                           lambda_range: Tuple[float, float], resolution: float) -> Tuple[float, Tuple[NDArray, NDArray]]:\n",
    "        num_iterations = int(np.rint((lambda_range[1] - lambda_range[0]) / resolution))\n",
    "        all_mse = np.empty(shape=(num_iterations, 2))\n",
    "        old_lambda = self._lambda\n",
    "        \n",
    "        best_lambda = lambda_range[0]\n",
    "        best_lambda_mse = np.inf\n",
    "        self._lambda = lambda_range[0]\n",
    "        iter = 0\n",
    "        while self._lambda < lambda_range[1] and iter < num_iterations:\n",
    "            self.fit(X_train, y_train)\n",
    "            curr_mse = self.mse(X_test, y_test)\n",
    "            if best_lambda_mse > curr_mse:\n",
    "                best_lambda = self._lambda\n",
    "                best_lambda_mse = curr_mse\n",
    "            all_mse[iter, 0] = self._lambda\n",
    "            all_mse[iter, 1] = curr_mse\n",
    "            self._lambda += resolution\n",
    "            iter += 1\n",
    "\n",
    "        self._lambda = old_lambda\n",
    "        return best_lambda, all_mse.T\n",
    "\n",
    "    def set_lambda(self, reg_lambda: float) -> None:\n",
    "        if reg_lambda < 0:\n",
    "            raise Exception('lambda must be >= 0')\n",
    "        self._lambda = reg_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce9272e0-5afc-4bb4-80c1-bcf76c4d6135",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier (Predictor):\n",
    "    '''\n",
    "    Implements a classifier model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(z: float) -> float:\n",
    "        z = np.clip(z, -600, 600) # for avoiding overflow\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid_d(z: float) -> float:\n",
    "        sig_z = sigmoid(z)\n",
    "        return sig_z * (1 - sig_z)\n",
    "\n",
    "    def classify(self, X: NDArray) -> NDArray:\n",
    "        if self._theta is None:\n",
    "            raise Exception('It is needed to fit model first')\n",
    "        else:\n",
    "            X = self._addOnesLeft(X)\n",
    "            return self.sigmoid(X.dot(self._theta))\n",
    "\n",
    "    def score(self, X: NDArray, y: NDArray) -> float:\n",
    "        n = len(y)\n",
    "        correct = 0\n",
    "        predictions = self.classify(X)\n",
    "        for i in range(n):\n",
    "            prediction = predictions[i]\n",
    "            correct_val = y[i]\n",
    "            if ((prediction >= 0.5) and (correct_val >= 0.5)) or ((prediction < 0.5) and (correct_val < 0.5)):\n",
    "                correct += 1\n",
    "        return correct / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcaefae0-2687-4b48-8504-1393e003e083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradDescReg (RegularizedModel, Classifier): # Diamond pattern, is fine because of MRO.\n",
    "    '''\n",
    "    Implements Gradient Descent with regularization\n",
    "    '''\n",
    "\n",
    "    # constructor\n",
    "    def __init__(self, max_iters: int = 1000, seed: int = 0, min_conv_diff: float = 10**-8, learning_rate: float = 0.001, reg_lambda_: float = 0.0) -> None:\n",
    "        self._max_iters = max_iters\n",
    "        self._min_conv_diff = min_conv_diff\n",
    "        self._alpha = learning_rate\n",
    "        super().__init__(reg_lambda=reg_lambda_)\n",
    "\n",
    "\n",
    "    # applies gradient descent with regularization\n",
    "    def fit(self, X : NDArray, y : NDArray) -> None:\n",
    "        # add a column of 1s\n",
    "        X_full = super()._addOnesLeft(X)\n",
    "\n",
    "        # initialize weights\n",
    "        #self._theta = np.zeros(X_full.shape[1]).reshape(-1, 1)\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self._theta = rng.standard_normal((X_full.shape[1], 1))\n",
    "\n",
    "        # setup learning rate\n",
    "        lr = self._alpha / X_full.shape[0]\n",
    "        \n",
    "        # setup regularization\n",
    "        reg = 1 - self._lambda * lr\n",
    "        \n",
    "        epoch = 0\n",
    "        conv_diff = self._min_conv_diff + 1\n",
    "        while (epoch < self._max_iters and conv_diff > self._min_conv_diff):\n",
    "            \n",
    "            # setup residuals\n",
    "            h = self.classify(X)\n",
    "            r = h - y\n",
    "        \n",
    "            # apply gradient descent with regularization (1 epoch)\n",
    "            new_theta = (reg * self._theta) - (lr * (X_full.T @ r))\n",
    "\n",
    "            # clipping for high learning rates\n",
    "            max_theta_value = 1e8\n",
    "            new_theta = np.clip(new_theta, -max_theta_value, max_theta_value)\n",
    "\n",
    "            # checks the norm of the difference in thetas and updates theta\n",
    "            conv_diff = np.linalg.norm(new_theta - self._theta)\n",
    "            self._theta = new_theta\n",
    "            \n",
    "            epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bce97454-a2eb-4fdd-b62c-b61a3d0d894c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to split dataset\n",
    "# PRE X.shape[0] == y.shape[0]\n",
    "def split_dataset(X, y, fit_percent=0.7):\n",
    "    \n",
    "    split_row = int(X.shape[0] * fit_percent)  \n",
    "    xf = X[0:split_row, :] \n",
    "    yf = y[0:split_row, :] \n",
    "    xt = X[split_row:, :]\n",
    "    yt = y[split_row:, :]\n",
    "    return xf, yf, xt, yt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a229c3b-2fe8-4dfe-bbfb-aec07d424ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to split the target vector of the matrix (last column)\n",
    "# WARNING: only works when there is only one target per feature\n",
    "def split_features_target(data):\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, data.shape[1] - 1].reshape(-1,1) # reshape to keep matrix-like format (2D instead of 1D)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07a3b085-6690-482f-be7f-a30be77eaf09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to split dataset randomly\n",
    "# PRE X.shape[0] == y.shape[0]\n",
    "def split_dataset_rng(X, y, fit_percent=0.7, seed=0):\n",
    "    rng = np.random.default_rng(seed) # 0 = random seed\n",
    "    data = np.column_stack((X, y))\n",
    "    rng.shuffle(data)\n",
    "\n",
    "    X, y = split_features_target(data)\n",
    "    return split_dataset(X, y, fit_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0577607b-1206-4713-90fd-bf168a030217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get all the prediction classes\n",
    "def get_prediction_classes(correct_y, predicted_y):\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    true_negatives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range(len(correct_y)):\n",
    "        correct = correct_y[i]\n",
    "        predicted = predicted_y[i]\n",
    "        if correct >= 0.5:\n",
    "            if predicted >= 0.5:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "        else:\n",
    "            if predicted >= 0.5:\n",
    "                false_negatives += 1\n",
    "            else:\n",
    "                true_negatives += 1\n",
    "    return true_positives, false_positives, true_negatives, false_negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cdca248-ad04-4f72-a685-7204f2c3b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate precision\n",
    "def precision(correct_y, predicted_y):\n",
    "    true_positives, false_positives, true_negatives, false_negatives = get_prediction_classes(correct_y, predicted_y)\n",
    "    return true_positives / (true_positives + false_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03175093-ab38-4f6a-a17b-48fc0eeef3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate recall\n",
    "def recall(correct_y, predicted_y):\n",
    "    true_positives, false_positives, true_negatives, false_negatives = get_prediction_classes(correct_y, predicted_y)\n",
    "    return true_positives / (true_positives + false_negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57aa8d59-3199-449d-9fc3-6547a8153500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to calculate f1 score\n",
    "def f1_score(correct_y, predicted_y):\n",
    "    p = precision(correct_y, predicted_y)\n",
    "    r = recall(correct_y, predicted_y)\n",
    "    return 2 * ((p * r) / (p + r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45d8af5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training \n",
      "\n",
      "Training Accuracy: 0.999750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_training = scipy.io.loadmat('spamTrain.mat')\n",
    "print('\\nTraining \\n')\n",
    "C = 0 # regularization coefficient\n",
    "\n",
    "X=data_training['X']\n",
    "y=data_training['y'].ravel()\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "seed = 137\n",
    "\n",
    "# hyperparameters\n",
    "data_split = 0.8\n",
    "lambda_range = [0.93, 0.95]\n",
    "lambda_step_precision = 0.001\n",
    "\n",
    "max_iters = 5000\n",
    "min_conv_diff = 10**-8\n",
    "learning_rate = 20.0\n",
    "reg_lambda = 0.949\n",
    "\n",
    "# learning\n",
    "y = y.reshape(-1, 1) # makes it a column vector\n",
    "model = GradDescReg(max_iters, seed, min_conv_diff, learning_rate, reg_lambda)\n",
    "\n",
    "#X_train, y_train, X_test, y_test = split_dataset_rng(X, y, data_split, seed)\n",
    "#best_lambda, all_mse = model.find_optimal_lambda(X_train, y_train, X_test, y_test, lambda_range, lambda_step_precision)\n",
    "#print(f\"Optimal lambda: {best_lambda}.\")\n",
    "#model.set_lambda(best_lambda)\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print('Training Accuracy: %f\\n' % model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bef5d6",
   "metadata": {},
   "source": [
    "After training the classifier, we can evaluate it on a test set. We have included a test set in spamTest.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0126d597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing \n",
      "\n",
      "Test Accuracy: 0.993000\n",
      "\n",
      "Test F1-score: 0.983713\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting \\n')\n",
    "\n",
    "#accuracy\n",
    "data_test=scipy.io.loadmat('spamNotebookBase/spamTest.mat')\n",
    "print('Test Accuracy: %f\\n' % model.score(data_test['Xtest'], data_test['ytest']))\n",
    "\n",
    "#F1-score\n",
    "yp = model.predict(data_test['Xtest'])\n",
    "print('Test F1-score: %f\\n' % f1_score(data_test['ytest'], yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb48eb6b-8080-437e-8a50-e2e08df539c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets: [[1 1 0 ... 1 0 0]]\n",
      "\n",
      "Spam email sample words:\n",
      "['accept', 'account', 'afford', 'all', 'an', 'and', 'anybodi', 'applic', 'approv', 'ar', 'as', 'beat', 'busi', 'by', 'card', 'click', 'consult', 'credit', 'deal', 'down', 'easi', 'fee', 'hour', 'httpaddr', 'is', 'it', 'low', 'mail', 'make', 'merchant', 'monei', 'no', 'not', 'number', 'oblig', 'obtain', 'of', 'offer', 'onli', 'order', 'our', 'pleas', 'rate', 'resid', 'retail', 'set', 'start', 'thi', 'to', 'todai', 'type', 'unsubscrib', 'up', 'us', 'we', 'will', 'within', 'your']\n",
      "\n",
      "Legit email sample words:\n",
      "['around', 'be', 'date', 'httpaddr', 'not', 'planet', 'possibl', 'sai', 'small', 'soon', 'star', 'suggest', 'suppli', 'url', 'will']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#show samples:\n",
    "def email_words(X):\n",
    "    m = len(X)\n",
    "    words = []\n",
    "    for i in np.arange(0, m):\n",
    "        if X[i] == 1:\n",
    "            words.append(vocabList[i])\n",
    "    return words    \n",
    "\n",
    "print('Targets: ', end=\"\")\n",
    "print(data_training['y'].T)\n",
    "print('\\nSpam email sample words:')\n",
    "print(email_words(data_training['X'][0,:]))\n",
    "print('\\nLegit email sample words:')\n",
    "print(email_words(data_training['X'][2,:]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae1d84-7ec5-4470-8e0b-6d1cdc3dfef9",
   "metadata": {},
   "source": [
    "**Top Predictors of Spam**\n",
    "\n",
    "We can inspect the weights learned by the model to understand better how it is determining\n",
    "whether an email is spam or not. The following code finds the words with\n",
    "the highest weights in the classifier. Informally, the classifier\n",
    "assign high credit to these words as the most likely indicators of spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "edf96fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 words indicators of spam: ['remov', 'our', 'click', 'will', 'guarante', 'basenumb', 'below', 'dollarnumb', 'bodi', 'hour']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Sort the weights and obtain the corresponding entries in the vocabulary list\n",
    "weights = np.copy(model.theta()).flatten()\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "words = []\n",
    "curr_max = -1\n",
    "curr_max_i = 0\n",
    "for j in range(10):\n",
    "    for i in range(1, len(weights)):\n",
    "        w = weights[i]\n",
    "        if w > curr_max:\n",
    "            curr_max = w\n",
    "            curr_max_i = i\n",
    "    words.append(vocabList[curr_max_i - 1])\n",
    "    weights[curr_max_i] = -1\n",
    "    curr_max = -1\n",
    "    curr_max_i = 0\n",
    "\n",
    "\n",
    "print('\\nTop 10 words indicators of spam: %s\\n' % words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08d467",
   "metadata": {},
   "source": [
    "**Try Your Own Emails**\n",
    "\n",
    "Now that you've trained the spam classifier, you can use it on your own\n",
    "emails! In the starter code, we have included spamSample1.txt,\n",
    "spamSample2.txt, emailSample1.txt and emailSample2.txt as examples.\n",
    "\n",
    "The following code reads in one of these emails and then uses your\n",
    "learned classifier to determine whether the email is Spam or Not Spam\n",
    "\n",
    "Set the file to be read in (change this to spamSample2.txt,\n",
    "mailSample1.txt or emailSample2.txt to see different predictions on\n",
    "different emails types). Try your own emails as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a6b1a154-dc12-4b85-a621-236612318b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "\n",
    "    # use model to predict if X email is SPAM or NOT\n",
    "    # if it is SPAM, return 1\n",
    "    # else return 0\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    #return model.classify(X.reshape(1, -1))\n",
    "    return 1 if model.classify(X.reshape(1, -1)) >= 0.5 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7aabc82e-f53b-478d-a2cc-b888d2468e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1 indicates spam, 0 indicates not spam)\n",
      "\n",
      "\n",
      "File:  spamNotebookBase/emailSample1.txt\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon know how much it cost to host a web portal well it depend on how mani \n",
      "visitor you re expect thi can be anywher from less than number buck a month \n",
      "to a coupl of dollar number you should checkout httpaddr or perhap amazon ec \n",
      "number if your run someth big to unsubscrib yourself from thi mail list send \n",
      "an email to emailaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "Spam Classification: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "File:  spamNotebookBase/emailSample2.txt\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "folk my first time post have a bit of unix experi but am new to linux just \n",
      "got a new pc at home dell box with window xp ad a second hard diskfor linux \n",
      "partit the disk and have instal suse number number from cd which wentfin \n",
      "except it didn t pick up my monitor i have a dell brand e numberfpp number \n",
      "lcd flat panel monitor and a nvidia geforc numberti number video card both of \n",
      "which are probabl too new to featur in suse s defaultset i download a driver \n",
      "from the nvidia websit and instal it use rpm then i ran sax number as wa \n",
      "recommend in some post i found on the net butit still doesn t featur my video \n",
      "card in the avail list what next anoth problem i have a dell brand keyboard \n",
      "and if i hit cap lock twice the whole machin crash in linux not window even \n",
      "the onoff switch isinact leav me to reach for the power cabl instead if anyon \n",
      "can help me in ani way with these prob i d be realli grate i ve search the \n",
      "net but have run out of idea or should i be go for a differ version of linux \n",
      "such as redhat opinionswelcom thank a lot peter irish linux user group \n",
      "emailaddr for unsubscript inform list maintain emailaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "Spam Classification: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "File:  spamNotebookBase/spamSample1.txt\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "do you want to make dollar number or more per week if you are a motiv and \n",
      "qualifi individu i will person demonstr to you a system that will make you \n",
      "dollar number number per week or more thi is not mlm call our number hour pre \n",
      "record number to get the detail number number number i need peopl who want to \n",
      "make seriou money make the call and get the fact invest number minut in \n",
      "yourself now number number number look forward to your call and i will \n",
      "introduc you to peopl like yourself whoar current make dollar number number \n",
      "plu per week number number numberljgv number numberlean numberlrm number \n",
      "numberwxho numberqiyt number numberrjuv numberhqcf number numbereidb \n",
      "numberdmtvl number \n",
      "\n",
      "=========================\n",
      "\n",
      "Spam Classification: 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "File:  spamNotebookBase/spamSample2.txt\n",
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "best buy viagra gener onlineviagra numbermg x number pill dollar number free \n",
      "pill reorder discount top sell number qualiti satisfact guaranteedw accept \n",
      "visa master e check payment number satisfi customershttpaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "Spam Classification: 1\n"
     ]
    }
   ],
   "source": [
    "#Test the predictor on sample emails\n",
    "\n",
    "filename0 = 'spamNotebookBase/emailSample1.txt'     #Should return 0: No spam\n",
    "filename1 = 'spamNotebookBase/emailSample2.txt'    #Should return 0: No spam    \n",
    "filename2 = 'spamNotebookBase/spamSample1.txt'     #Should return 1: SPAM\n",
    "filename3 = 'spamNotebookBase/spamSample2.txt'     #Should return 1: SPAM\n",
    "\n",
    "\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')\n",
    "\n",
    "# Read and predict\n",
    "print('File: ', filename0);\n",
    "file_contents0 = readFile(filename0)\n",
    "word_indices0 = processEmail(file_contents0)\n",
    "x0 = emailFeatures(word_indices0)\n",
    "p0 = predict(model, x0)\n",
    "print('Spam Classification:', p0)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print('File: ', filename1);\n",
    "file_contents1 = readFile(filename1)\n",
    "word_indices1 = processEmail(file_contents1)\n",
    "x1 = emailFeatures(word_indices1)\n",
    "p1 = predict(model, x1)\n",
    "print('Spam Classification:', p1)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print('File: ', filename2);\n",
    "file_contents2 = readFile(filename2)\n",
    "word_indices2 = processEmail(file_contents2)\n",
    "x2 = emailFeatures(word_indices2)\n",
    "p2 = predict(model, x2)\n",
    "print('Spam Classification:', p2)\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "print('File: ', filename3);\n",
    "file_contents3 = readFile(filename3)\n",
    "word_indices3 = processEmail(file_contents3)\n",
    "x3 = emailFeatures(word_indices3)\n",
    "p3 = predict(model, x3)\n",
    "print('Spam Classification:', p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810d20c",
   "metadata": {},
   "source": [
    "**Credits** \n",
    "\n",
    "This lab assignment is based on an Octave programming project of the course Machine Learning from Coursera[2]. \n",
    "\n",
    "References\n",
    "* [1] http://spamassassin.apache.org/publiccorpus/\n",
    "* [2] http://www.coursera.org/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
