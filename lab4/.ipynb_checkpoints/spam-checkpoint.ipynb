{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dc9d331",
   "metadata": {},
   "source": [
    "#### <h1>Machine Learning</h1>\n",
    "\n",
    "<h2>Lab assignment 4: Spam filter</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc93de75-0f55-4a10-965e-c9cc62689abc",
   "metadata": {},
   "source": [
    "Current email services provide spam filters that are able to classify emails into spam and non-spam email with high accuracy. In the following you experiment known classifiers to build your own spam filter.\n",
    "\n",
    "The goal is to discriminate whether a given email, $x$, is spam ($y$=1) or non-spam ($y$=0). For this you need to convert each email into a feature vector $\\vec{x} \\in \\{0, 1\\}^n$. The following will walk you through how such a feature vector can be constructed from an email.\n",
    "\n",
    "Throughout the rest of this lab, you will be using the datasets included which are a subset of the Spam Assassin Public Corpus[1]. For the purpose of this lab, you will only be using the body of the email (excluding the email headers)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34c72c83",
   "metadata": {},
   "source": [
    "![sampleEmail](sampleEmail.png \"sampleEmail\") \\\n",
    "Fig 1. Sample email"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a887bdae-48e9-4ea3-88e5-e02ed436c236",
   "metadata": {},
   "source": [
    "\\\n",
    "Before starting a machine learning task, it is usually useful to take a look at examples from the dataset. For this we need to import some packages and a write a little readFile function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f6a3969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import operator\n",
    "import csv\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60ef9",
   "metadata": {},
   "source": [
    "Now we read the text file with the email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ad4ae9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original email\n",
      "> Anyone knows how much it costs to host a web portal ?>Well, it depends on how many visitors you're expecting.This can be anywhere from less than 10 bucks a month to a couple of $100. You should checkout http://www.rackspace.com/ or perhaps Amazon EC2 if youre running something big..To unsubscribe yourself from this mailing list, send an email to:groupname-unsubscribe@egroups.com\n"
     ]
    }
   ],
   "source": [
    "def readFile(filename = None):\n",
    "    #READFILE reads a file and returns its entire contents\n",
    "    #   file_contents = READFILE(filename) reads a file and returns its entire\n",
    "    #   contents in file_contents\n",
    "    with open(filename, 'r') as file:\n",
    "        file_contents = file.read().replace('\\n', '')\n",
    "    return file_contents\n",
    "\n",
    "\n",
    "file_contents = readFile('emailSample1.txt')\n",
    "\n",
    "print('Original email')\n",
    "print(file_contents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeecb8f8",
   "metadata": {},
   "source": [
    "**Preprocessing Emails**\n",
    "\n",
    "The above is a sample email that contains a URL, an email address (at the end), numbers, and dollar amounts. While many emails would contain similar types of entities (e.g., numbers, other URLs, or other email addresses), the specific entities (e.g., the specific URL or specific dollar amount) will be different in almost every email. Therefore, one method often employed in processing emails is to “normalize” these values, so that all URLs are treated the same, all numbers are treated the same, etc. For example, we could replace each URL in the email with the unique string “httpaddr\" to indicate that a URL was present. This has the effect of letting the spam classifier make a classification decision based on whether any URL was present, rather than whether a specific URL was present. This typically improves the performance of a spam classifier, since spammers often randomize the URLs, and thus the odds of seeing any particular URL again in a new piece of spam is very small.\n",
    "\n",
    "In the function processEmail, we have implemented the following email preprocessing and normalization steps:\n",
    "\n",
    "* Lower-casing: The entire email is converted into lower case, so that capitalization is ignored (e.g., IndIcaTE is treated the same as Indicate).\n",
    "* Stripping HTML: All HTML tags are removed from the emails. Many emails often come with HTML formatting; we remove all the HTML tags, so that only the content remains.\n",
    "* Normalizing URLs: All URLs are replaced with the text “httpaddr\". \n",
    "* Normalizing Email Addresses: All email addresses are replaced with the text “emailaddr\".\n",
    "* Normalizing Numbers: All numbers are replaced with the text “number\".\n",
    "* Normalizing Dollars: All dollar signs ($) are replaced with the text “dollar\".\n",
    "* Word Stemming: Words are reduced to their stemmed form. For example, “discount\", “discounts\", “discounted\" and “discounting\" are all replaced with “discount\". Sometimes, the Stemmer actually strips off additional characters from the end, so “include\", “includes\", “included\", and “including\" are all replaced with “includ\".\n",
    "* Removal of non-words: Non-words and punctuation have been removed. All white spaces (tabs, newlines, spaces) have all been trimmed to a single space character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22909b22",
   "metadata": {},
   "source": [
    "**Vocabulary List**\n",
    "\n",
    "After preprocessing the emails, we have a list of word for each email. The next step is to choose which words we would like to use in our classifier and which we would want to leave out.\n",
    "\n",
    "For this lab assignment, we have chosen only the most frequently occurring words as our set of words considered (the vocabulary list). Since words that occur rarely in the training set are only in a few emails, they might cause the model to overfit our training set. The complete vocabulary list is in the file vocab.txt.\n",
    "\n",
    "Our vocabulary list was selected by choosing all words which occur at least a 100 times in the spam corpus, resulting in a list of 1899 words. In practice, a vocabulary list with about 10,000 to 50,000 words is often used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36752c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get Vocabulary\n",
    "\n",
    "def getVocabList():\n",
    "    vocabList = [' ' for i in range(1899)]\n",
    "    with open('vocab.txt') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter='\\t')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            vocabList[line_count] = row[1]\n",
    "            line_count += 1\n",
    "    return vocabList\n",
    "\n",
    "vocabList = getVocabList()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c57f9a-0c2d-4c06-8a4a-eb7d7376d16c",
   "metadata": {},
   "source": [
    "---\n",
    "# Quiz\n",
    "\\\n",
    "From this point on, you will be asked to complete the code of some functions.\n",
    "\n",
    "The instructions to do so, are defined as comments, in the place where the code should be inserted, started by the word:\n",
    "\n",
    "**# Instructions:**\n",
    "\n",
    "The code must be inserted below the instructions, after the comment line:\n",
    "\n",
    "&#35; ====================== YOUR CODE HERE ====================== \n",
    "\n",
    "\\\n",
    "Take a look at the example below, function `processEmail()`. This function converts the email text into stemmed words, and then into a vector of vocables indexes, named **word_indices**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c2bf7b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Processed Email ====\n",
      "\n",
      "\n",
      "anyon know how much it cost to host a web portal well it depend on how mani \n",
      "visitor you re expect thi can be anywher from less than number buck a month \n",
      "to a coupl of dollar number you should checkout httpaddr or perhap amazon ec \n",
      "number if your run someth big to unsubscrib yourself from thi mail list send \n",
      "an email to emailaddr \n",
      "\n",
      "=========================\n",
      "\n",
      "Word Indices:  [  86.  916.  794. 1077.  883.  370. 1699.  790. 1822. 1831.  883.  431.\n",
      " 1171.  794. 1002. 1893. 1364.  592. 1676.  238.  162.   89.  688.  945.\n",
      " 1663. 1120. 1062. 1699.  375. 1162.  477. 1120. 1893. 1510.  799. 1182.\n",
      " 1237.  512. 1120.  810. 1895. 1440. 1547.  181. 1699. 1758. 1896.  688.\n",
      " 1676.  992.  961. 1477.   71.  530. 1699.  531.]\n"
     ]
    }
   ],
   "source": [
    "def processEmail(email_contents = None):\n",
    "    #   word_indices = PROCESSEMAIL(email_contents) preprocesses\n",
    "    #   the body of an email and returns a list of indices of the\n",
    "    #   words contained in the email.\n",
    "\n",
    "    # ========================== Preprocess Email ===========================\n",
    "    # Headers\n",
    "    # Handle them bellow  if you are working with raw emails with the\n",
    "    # full headers\n",
    "\n",
    "    # Lower case\n",
    "    email_contents = email_contents.lower()\n",
    "\n",
    "    # Strip all HTML\n",
    "    # Looks for any expression that starts with < and ends with > and replace\n",
    "    # it with a space\n",
    "    pattern = '<[^<]+?>'\n",
    "    email_contents = re.sub(pattern, ' ', email_contents)\n",
    "\n",
    "    # Look for one or more characters between 0-9\n",
    "    pattern = r'[0-9]+'\n",
    "    # Match all digits in the string and replace them with 'number'\n",
    "    email_contents = re.sub(pattern, ' number', email_contents)\n",
    "\n",
    "    # Handle URLS\n",
    "    # Look for strings starting with http:// or https://\n",
    "    pattern=r'(http|https)\\S+'\n",
    "    email_contents = re.sub(pattern, 'httpaddr', email_contents)\n",
    "\n",
    "    # Handle Email Addresses\n",
    "    # Look for strings with @ in the middle\n",
    "    pattern = r'[\\w.+-]+@[\\w-]+\\.[\\w.-]+'\n",
    "    email_contents = re.sub(pattern, 'emailaddr', email_contents)\n",
    "\n",
    "    pattern = r'\\$'\n",
    "    email_contents = re.sub(pattern, 'dollar', email_contents)\n",
    "\n",
    "    # ========================== Tokenize Email ===========================\n",
    "\n",
    "    # Output the email to screen as well\n",
    "    print('\\n==== Processed Email ====\\n\\n' % ())\n",
    "    # Process file\n",
    "    l = 0\n",
    "    \n",
    "    # Init return value\n",
    "    word_indices = np.array([])\n",
    "    for s in re.split(\"[ .:;\\\\-,']\",email_contents):\n",
    "        # Tokenize and also get rid of any punctuation\n",
    "        s = re.sub(r'[^\\w\\s]','', s)\n",
    "        # Remove any non alphanumeric characters\n",
    "        s = re.sub('[^0-9a-zA-Z]+', ' ', s)\n",
    "\n",
    "        # Stem the word\n",
    "        ps = nltk.stem.PorterStemmer()\n",
    "        s=ps.stem(s)\n",
    "\n",
    "        # Skip the word if it is too short\n",
    "        if len(s) < 1:\n",
    "            continue\n",
    "        # Look up the word in the dictionary and add to word_indices if\n",
    "        # found\n",
    "        \n",
    "        # ====================== YOUR CODE HERE ======================\n",
    "        # Instructions: Fill in this function to add the index of str to\n",
    "        #               word_indices if it is in the vocabulary. At this point\n",
    "        #               of the code, you have a stemmed word from the email in\n",
    "        #               the variable str. You should look up str in the\n",
    "        #               vocabulary list (vocabList). If a match exists, you\n",
    "        #               should add the index of the word to the word_indices\n",
    "        #               vector. Concretely, if str = 'action', then you should\n",
    "        #               look up the vocabulary list to find where in vocabList\n",
    "        #               'action' appears. For example, if vocabList{18} =\n",
    "        #               'action', then, you should add 18 to the word_indices\n",
    "        #               vector.\n",
    "       \n",
    "        for i in np.arange(0, len(vocabList)) :\n",
    "            if (vocabList[i] == s):\n",
    "                word_indices = np.append(word_indices, i+1)\n",
    "\n",
    "        # =============================================================\n",
    "        # Print to screen, ensuring that the output lines are not too long\n",
    "        if (l + len(s) + 1) > 78:\n",
    "            print()\n",
    "            l = 0\n",
    "        print('%s ' % (s),end='')\n",
    "        l = l + len(s) + 1\n",
    "        \n",
    "    # Print footer\n",
    "    print('\\n\\n=========================\\n' % ())\n",
    "    return word_indices\n",
    "\n",
    "word_indices = processEmail(file_contents)\n",
    "# Print Stats\n",
    "print('Word Indices: ', word_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ccac9",
   "metadata": {},
   "source": [
    "The above is the processed sample email. While preprocessing has left word fragments and non-words, this form turns out to be much easier to work with for performing feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a227c13",
   "metadata": {},
   "source": [
    "Given the vocabulary list, we can now map each word in the preprocessed emails into a list of word indices that contains the index of the word in the vocabulary list. For example, in the sample email, the word “anyone\" was first normalized to “anyon\" and then mapped onto the index 86 in the vocabulary list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c608d8d",
   "metadata": {},
   "source": [
    "**Your first task**\n",
    "\n",
    "Your first task is to complete the code in emailFeatures(word_indices = None) below that takes in a word_indices array and produces a feature vector from the word indices. In other words, you will now implement the feature extraction that converts each email into a vector in $\\{0, 1\\}^n$. For this exercise, you will be using $n$ = number of words in vocabulary list. Specifically, the feature $x_i \\in \\{0,1 \\}$ for an email corresponds to whether the $i$-th word in the dictionary occurs in the email. That is, $x_i=1$ if the $i$-th word is in the email and $x_i=0$ if the $i$-th word is not present in the email. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "978cd9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices = None):\n",
    "    #   x = EMAILFEATURES(word_indices) takes in a word_indices vector and\n",
    "    #   produces a feature vector from the word indices.\n",
    "\n",
    "    # Total number of words in the dictionary\n",
    "    n = len(vocabList)\n",
    "\n",
    "    # You need to return the following variables correctly.\n",
    "    x = np.zeros(n)\n",
    "  \n",
    "\n",
    "    # Instructions: Fill in this function to return a feature vector for the\n",
    "    #               given email (word_indices). To help make it easier to\n",
    "    #               process the emails, we have have already pre-processed each\n",
    "    #               email and converted each word in the email into an index in\n",
    "    #               a fixed dictionary (of 1899 words). The variable\n",
    "    #               word_indices contains the list of indices of the words\n",
    "    #               which occur in one email.\n",
    "\n",
    "    #               Concretely, if an email has the text:\n",
    "    #                  The quick brown fox jumped over the lazy dog.\n",
    "    #               Then, the word_indices vector for this text might look\n",
    "    #               like:\n",
    "    #                   60  100   33   44   10     53  60  58   5\n",
    "    #               where, we have mapped each word onto a number, for example:\n",
    "    #                   the   -- 60\n",
    "    #                   quick -- 100\n",
    "    #                   ...\n",
    "    #              (note: the above numbers are just an example and are not the\n",
    "    #               actual mappings).\n",
    "\n",
    "    #              Your task is take one such word_indices vector and construct\n",
    "    #              a binary feature vector that indicates whether a particular\n",
    "    #              word occurs in the email. That is, x(i) = 1 when word i\n",
    "    #              is present in the email. Concretely, if the word 'the' (say,\n",
    "    #              index 60) appears in the email, then x(60) = 1. The feature\n",
    "    #              vector should look like:\n",
    "\n",
    "    #              x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..];\n",
    "\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    for word_index in word_indices:\n",
    "        x[int(word_index)] = 1.0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678d352c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting features from sample email (emailSample1.txt)\n",
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mExtracting features from sample email (emailSample1.txt)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m features = \u001b[43memailFeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Print Stats\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLength of feature vector: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m % \u001b[38;5;28mlen\u001b[39m(features))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36memailFeatures\u001b[39m\u001b[34m(word_indices)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Instructions: Fill in this function to return a feature vector for the\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m#               given email (word_indices). To help make it easier to\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m#               process the emails, we have have already pre-processed each\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ====================== YOUR CODE HERE ======================\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word_index \u001b[38;5;129;01min\u001b[39;00m word_indices:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_index\u001b[49m\u001b[43m]\u001b[49m = \u001b[32m1.0\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mIndexError\u001b[39m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "print('\\nExtracting features from sample email (emailSample1.txt)\\n')\n",
    "features = emailFeatures(word_indices)\n",
    "# Print Stats\n",
    "print('Length of feature vector: %d' % len(features))\n",
    "print('Number of non-zero entries: %d \\n' % sum(features > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddbc163",
   "metadata": {},
   "source": [
    "**Second task**\n",
    "\n",
    "After you have completed the feature extraction functions, the next step will load a preprocessed training dataset that will be used to train a classifier. **spamTrain.mat** contains 4000 training examples of spam and non-spam email, while **spamTest.mat** contains 1000 test examples. Each original email was processed using the processEmail and emailFeatures functions and converted into a vector $x \\in \\{0, 1\\}^{1899}$. After loading the dataset, train a classifier of your choice for discriminating between spam ($y=1$) and non-spam ($y=0$) emails. \n",
    "\n",
    "Your current task is to train this classifier and record the achieved training accuracies in both the training and the test sets. It is recommended to regularize your classifier. You can use either your own developed classifier code or sklearn classifiers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_training = scipy.io.loadmat('spamTrain.mat')\n",
    "print('\\nTraining \\n')\n",
    "C = 0 # regularization coefficient\n",
    "\n",
    "X=data_training['X']\n",
    "y=data_training['y'].ravel()\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "\n",
    "...\n",
    "print('Training Accuracy: %f\\n' % model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79bef5d6",
   "metadata": {},
   "source": [
    "After training the classifier, we can evaluate it on a test set. We have included a test set in spamTest.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0126d597",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTesting \\n')\n",
    "\n",
    "#accuracy\n",
    "data_test=scipy.io.loadmat('spamNotebookBase/spamTest.mat')\n",
    "print('Test Accuracy: %f\\n' % model.score(data_test['Xtest'], data_test['ytest']))\n",
    "\n",
    "#F1-score\n",
    "yp = model.predict(data_test['Xtest'])\n",
    "print('Test F1-score: %f\\n' % f1_score(data_test['ytest'], yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb48eb6b-8080-437e-8a50-e2e08df539c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show samples:\n",
    "def email_words(X):\n",
    "    m = len(X)\n",
    "    words = []\n",
    "    for i in np.arange(0, m):\n",
    "        if X[i] == 1:\n",
    "            words.append(vocabList[i])\n",
    "    return words    \n",
    "\n",
    "print('Targets: ', end=\"\")\n",
    "print(data_training['y'].T)\n",
    "print('\\nSpam email sample words:')\n",
    "print(email_words(data_training['X'][0,:]))\n",
    "print('\\nLegit email sample words:')\n",
    "print(email_words(data_training['X'][2,:]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae1d84-7ec5-4470-8e0b-6d1cdc3dfef9",
   "metadata": {},
   "source": [
    "**Top Predictors of Spam**\n",
    "\n",
    "We can inspect the weights learned by the model to understand better how it is determining\n",
    "whether an email is spam or not. The following code finds the words with\n",
    "the highest weights in the classifier. Informally, the classifier\n",
    "assign high credit to these words as the most likely indicators of spam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf96fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the weights and obtain the corresponding entries in the vocabulary list\n",
    "weights = model.coef_\n",
    "\n",
    "# ====================== YOUR CODE HERE ======================\n",
    "....\n",
    "\n",
    "print('\\nTop 10 words indicators of spam: %s\\n' % words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08d467",
   "metadata": {},
   "source": [
    "**Try Your Own Emails**\n",
    "\n",
    "Now that you've trained the spam classifier, you can use it on your own\n",
    "emails! In the starter code, we have included spamSample1.txt,\n",
    "spamSample2.txt, emailSample1.txt and emailSample2.txt as examples.\n",
    "\n",
    "The following code reads in one of these emails and then uses your\n",
    "learned classifier to determine whether the email is Spam or Not Spam\n",
    "\n",
    "Set the file to be read in (change this to spamSample2.txt,\n",
    "mailSample1.txt or emailSample2.txt to see different predictions on\n",
    "different emails types). Try your own emails as well!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b1a154-dc12-4b85-a621-236612318b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, X):\n",
    "\n",
    "    # use model to predict if X email is SPAM or NOT\n",
    "    # if it is SPAM, return 1\n",
    "    # else return 0\n",
    "    # ====================== YOUR CODE HERE ======================\n",
    "    ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aabc82e-f53b-478d-a2cc-b888d2468e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the predictor on sample emails\n",
    "\n",
    "filename = 'spamNotebookBase/emailSample1.txt'     #Should return 0: No spam\n",
    "#filename = 'spamNotebookBase/emailSample2.txt'    #Should return 0: No spam    \n",
    "#filename = 'spamNotebookBase/spamSample1.txt'     #Should return 1: SPAM\n",
    "#filename = 'spamNotebookBase/spamSample2.txt'     #Should return 1: SPAM\n",
    "\n",
    "# Read and predict\n",
    "file_contents = readFile(filename)\n",
    "word_indices = processEmail(file_contents)\n",
    "x = emailFeatures(word_indices)\n",
    "p = predict(model, x)\n",
    "print('File: ', filename);\n",
    "print('Spam Classification:', p)\n",
    "print('(1 indicates spam, 0 indicates not spam)\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810d20c",
   "metadata": {},
   "source": [
    "**Credits** \n",
    "\n",
    "This lab assignment is based on an Octave programming project of the course Machine Learning from Coursera[2]. \n",
    "\n",
    "References\n",
    "* [1] http://spamassassin.apache.org/publiccorpus/\n",
    "* [2] http://www.coursera.org/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
