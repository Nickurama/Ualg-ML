{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02972eec",
   "metadata": {},
   "source": [
    "# Lab 2: Linear regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda23db",
   "metadata": {},
   "source": [
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38082d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from   scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3152f",
   "metadata": {},
   "source": [
    "Load the data set in the companion **demodataset.csv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e1e4751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  -0.878,  -32.208],\n",
       "       [   1.36 ,   49.178],\n",
       "       [   1.64 ,   63.384],\n",
       "       [   0.542,   50.955],\n",
       "       [   0.825,   76.403],\n",
       "       [  -0.596,  -90.715],\n",
       "       [  -0.056,   30.239],\n",
       "       [  -0.132,  -29.705],\n",
       "       [  -2.435, -139.298],\n",
       "       [  -0.404,  -10.483],\n",
       "       [   0.382,   34.85 ],\n",
       "       [   1.367,  103.15 ],\n",
       "       [  -0.748,  -25.526],\n",
       "       [  -0.382,    7.506],\n",
       "       [   0.842,   52.244],\n",
       "       [  -0.077,    7.081],\n",
       "       [   0.433,   76.325],\n",
       "       [  -0.217,  -52.17 ],\n",
       "       [   1.468,   71.62 ],\n",
       "       [   0.113,   -0.239],\n",
       "       [   1.731,   17.363],\n",
       "       [  -0.336,  -19.43 ],\n",
       "       [  -1.099,  -39.712],\n",
       "       [  -0.842,  -71.2  ],\n",
       "       [   0.566,   25.936],\n",
       "       [  -1.058,  -65.185],\n",
       "       [   0.695,    7.989],\n",
       "       [   0.539,   25.539],\n",
       "       [   0.502,   55.364],\n",
       "       [  -1.439,  -83.49 ],\n",
       "       [  -0.189,   -2.155],\n",
       "       [  -0.074,    4.711],\n",
       "       [  -1.793,  -19.926],\n",
       "       [   2.231,  105.28 ],\n",
       "       [   0.042,  -25.124],\n",
       "       [   1.174,   91.489],\n",
       "       [  -1.385,  -71.334],\n",
       "       [   0.216,    7.593],\n",
       "       [  -0.091,    1.128],\n",
       "       [   1.278,   74.053],\n",
       "       [  -2.136, -119.696],\n",
       "       [  -0.236,   -6.711],\n",
       "       [  -1.859,  -78.132],\n",
       "       [   0.326,    0.469],\n",
       "       [   0.   ,   -2.057],\n",
       "       [  -0.376,  -26.259],\n",
       "       [   1.041,   97.458],\n",
       "       [  -0.339,  -36.053],\n",
       "       [  -0.678,  -16.969],\n",
       "       [   1.218,   48.642],\n",
       "       [  -1.245,  -51.292],\n",
       "       [  -0.829,  -21.41 ],\n",
       "       [  -0.419,  -16.698],\n",
       "       [   0.38 ,   41.286],\n",
       "       [  -1.868, -101.572],\n",
       "       [   1.584,  110.486],\n",
       "       [   0.611,   31.714],\n",
       "       [   1.   ,   61.329],\n",
       "       [  -0.909,  -55.964],\n",
       "       [  -0.04 ,  -29.139],\n",
       "       [  -0.019,   34.499],\n",
       "       [  -1.421, -104.181],\n",
       "       [   0.351,   -6.904],\n",
       "       [  -0.989,  -42.908],\n",
       "       [   0.046,   -2.612],\n",
       "       [  -0.381,   22.904],\n",
       "       [   0.735,  101.816],\n",
       "       [  -0.638,  -41.195],\n",
       "       [  -2.659, -169.618],\n",
       "       [  -0.417,  -12.608],\n",
       "       [   0.257,   49.311],\n",
       "       [   0.37 ,   81.904],\n",
       "       [  -0.635,  -46.342],\n",
       "       [   0.048,  -34.044],\n",
       "       [  -2.344, -146.528],\n",
       "       [   0.508,   63.253],\n",
       "       [  -0.653,  -85.248],\n",
       "       [  -0.844,  -96.512],\n",
       "       [  -0.462,  -32.78 ],\n",
       "       [   0.204,   41.692],\n",
       "       [   0.503,   -0.585],\n",
       "       [  -0.269,   32.601],\n",
       "       [  -1.118,  -88.994],\n",
       "       [   0.524,   32.244],\n",
       "       [   0.088,   27.092],\n",
       "       [   0.551,   -9.631],\n",
       "       [  -2.033, -141.774],\n",
       "       [  -0.314,  -29.582],\n",
       "       [   1.175,   76.607],\n",
       "       [   0.066,    9.216],\n",
       "       [   1.248,   18.674],\n",
       "       [  -1.188,  -80.137],\n",
       "       [   1.407,  115.774],\n",
       "       [  -0.156,  -34.323],\n",
       "       [   2.292,  166.74 ],\n",
       "       [  -1.738, -127.151],\n",
       "       [  -2.04 , -103.446],\n",
       "       [   0.771,    5.669],\n",
       "       [   0.009,  -36.793],\n",
       "       [  -0.153,   -5.789]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.loadtxt(\"demodataset.csv\", delimiter=\",\")\n",
    "\n",
    "np.set_printoptions(precision=5)\n",
    "np.set_printoptions(suppress=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b43579-e757-4623-9d2c-1a81a10b614c",
   "metadata": {},
   "source": [
    "---\n",
    "Extract the first column in the data set to vector **x** and the second column to vector **y**.   \n",
    "\n",
    "Plot a figure showing the dataset as: { $(x^{(i)},(x^{(i)}) | i=1,…, m $}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03613673",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "x=data[:,0] # x is the first  column in the data set\n",
    "y=data[:,1] # y is the second column in the data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93036df0-ad23-47b4-90bd-0806ac132f39",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "1. a) Compute $θ_0^*$ and $θ_1^*$ for the line of best fit. \\\n",
    "\\\n",
    "i) using scipy.stats.linregress():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59002575-d1ae-4f4b-b79c-46d5eb656428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Θ0 (intercept) = 0.802\n",
      "Θ1 (slope)     = 56.758\n"
     ]
    }
   ],
   "source": [
    "Θ1, Θ0, r, p, std_err = stats.linregress(x, y)  # theta1 = slope, theta0 = intercept y axis\n",
    "print('Θ0 (intercept) = {:.3f}'.format(Θ0))\n",
    "print('Θ1 (slope)     = {:.3f}'.format(Θ1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5cdaf54-5b57-4ce0-9509-ccd4e88a7b8e",
   "metadata": {},
   "source": [
    "1 a) \\\n",
    "\\\n",
    "ii)\timplementing the linear regression model below using numpy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1e7a8-f584-4583-9c3a-9b7ce763b99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1dc541-b6e6-4258-8860-9cc1fc25bb5d",
   "metadata": {},
   "source": [
    "1 a)\\\n",
    "\\\n",
    "iii)\tcheck that i) and ii) produce the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feec09b-d378-45d4-acb2-f38b0627b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1c5264-992e-4a20-8457-1741403ecfdc",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "---\n",
    "1. b) Superimpose the line of best fit to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91579333-d711-43fc-aa01-0c0b426d1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313b03c9-72f6-40da-97e2-436a73d926b1",
   "metadata": {},
   "source": [
    "---\n",
    "1. c)\tPredict y for x = 2.231. What is the corresponding residual? Superimpose on the graphic obtained in b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334ab76b-f802-49d0-af12-03d97ab35fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1c82c-c35f-466c-9786-1f3cdd70b41a",
   "metadata": {},
   "source": [
    "---\n",
    "2. a) \\\n",
    "\\\n",
    "i) Compute $J(θ_0^*, θ_1^*)$ for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d7f62c-c710-4340-a6f4-aaa5d4d24c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be083319-e819-4da6-953f-ed6fa1812d05",
   "metadata": {},
   "source": [
    "---\n",
    "2. a)\\\n",
    "\\\n",
    "i) Express the cost function: \\\n",
    "\\\n",
    " $\\qquad J(θ_0, \\dots , θ_n) = \\frac{1}{2m}\\sum_{i=1}^{m} (h_θ(\\mathbf x^{(i)}) - y^{(i)})^2$ \\\n",
    "\\\n",
    "in vectorial notation and compute it for the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a40230-767c-4a3a-914a-57f623dfef9f",
   "metadata": {},
   "source": [
    "- Considering a dataset with $m$ data points (or samples), $n$ features and 1 target or label. \\\n",
    "$\\mathbf X$ matrix has $m$ rows, one for each data point and $1 + n$ columns, the left most for $bias$ and the rest for the features. \\\n",
    "$\\mathbf y$ vector has $m$ rows, one for each target: \\\n",
    "\\\n",
    "$\n",
    "\\qquad \n",
    "\\begin{split}\n",
    "\\mathbf{X}_{m\\times (1+n)} &=\n",
    "\\begin{bmatrix}\n",
    "    1 & x_1^{(1)}   & x_2^{(1)} & \\dots & x_n^{(1)}\\\\\n",
    "    1 & x_1^{(2)}   & x_2^{(2)} & \\dots & x_n^{(2)}\\\\\n",
    "    & & \\vdots & &\\\\\n",
    "    1 & x_1^{(m)} & x_2^{(m)} & \\dots & x_n^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\quad\\quad\n",
    "\\mathbf{y} &=\n",
    "\\begin{bmatrix}\n",
    "    y^{(1)}\\\\\n",
    "    y^{(2)}\\\\\n",
    "    \\vdots \\\\\n",
    "    y^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "\\end{split} \n",
    "$\n",
    "\n",
    "$\\qquad$ and vector $\\boldsymbol\\theta$ for $n$ coefficients:\n",
    "\n",
    "$\n",
    "\\qquad\n",
    "\\boldsymbol{\\theta} = \\begin{bmatrix}\n",
    "θ_0 \\\\ θ_1 \\\\ \\vdots \\\\ \\theta_n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "- The hypothesis function, for each $h^{(i)}$, where\n",
    "$\n",
    "\\mathbf x^{(i)} = \\begin{bmatrix}\n",
    "1 & x_1^{(i)} & \\dots & x_n^{(i)}\n",
    "\\end{bmatrix}\n",
    "$\n",
    ", given by: \\\n",
    "\\\n",
    "$$h_\\theta(\\mathbf x^{(i)}) = θ_0.1 + θ_1x_1^{(i)} + θ_2x_2^{(i)} + \\dots + θ_2x_n^{(i)} $$\n",
    "\n",
    "-  can be rewritten in vector form as the dot product of:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "{h_\\theta}(\\mathbf x^{(i)}) &= \n",
    "\\begin{bmatrix}\n",
    "1 & x_1^{(i)} & \\dots & x_n^{(i)}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "θ_0 \\\\ θ_1 \\\\ \\vdots \\\\ \\theta_n\n",
    "\\end{bmatrix} \\\\\n",
    "{h_\\theta}(\\mathbf x^{(i)}) &= \\mathbf x^{(i)}\\boldsymbol{\\theta}\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6e3df4-47b7-46a0-bd12-32adbfb8c2ac",
   "metadata": {},
   "source": [
    "- and for $m$ data points, as:\n",
    "$$\n",
    "\\begin{split}\n",
    "h_θ(\\mathbf{X}) &= \\mathbf{X}\\boldsymbol{\\theta}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- this will compute vector $\\mathbf{h}$ with $m$ elements:\n",
    "  \n",
    "$$\\mathbf{h} = \\mathbf{X}\\boldsymbol{\\theta}$$\n",
    "\n",
    "- Now, expressing the cost (or loss) function: \n",
    "\n",
    "$$J(θ_0, \\dots , θ_n) = \\frac{1}{2m}\\sum_{i=1}^{m} (h_θ(\\mathbf x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "- in vector notation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "J(θ_0, \\dots , θ_n) &= \\frac{1}{2m} (\\mathbf{h} - \\mathbf{y})^2 \\\\\n",
    "J(θ_0, \\dots , θ_n) &= \\frac{1}{2m} (\\mathbf{h} - \\mathbf{y})^T(\\mathbf{h} - \\mathbf{y})\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- computing the residuals vector with:\n",
    "\n",
    "$\\qquad \\qquad \\mathbf{r} = \\mathbf{h} - \\mathbf{y}$\n",
    "\n",
    "$$\n",
    "  J(θ) = J(θ_0, \\dots, θ_n) = \\color{red}{\\text{Enter final answer here}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d4671-55e9-48df-a35a-a351c4a735b0",
   "metadata": {},
   "source": [
    "---\n",
    "compute $J(θ_0^*, θ_1^*)$ in vector notation, for the given dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a466937-9368-46f5-be19-6201ef57348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569fe19-3378-4a1a-ab88-d0943419a89e",
   "metadata": {},
   "source": [
    "---\n",
    "2. a)\\\n",
    "\\\n",
    "ii)\tIs there any advantage to computing in vector notation with NumPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51329124-a227-462e-baee-c87ac3273efc",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Enter answer here:}}$\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c7cbb9-6519-4408-94f4-faa7eaedaf36",
   "metadata": {},
   "source": [
    "---\n",
    "2. a)\\\n",
    "\\\n",
    "iii)\tShow that point (average of X, average of Y) belongs to the line of best fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8e881-1dea-4659-9fba-c15fa1587658",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Write the math proof here:}}$\n",
    "\n",
    "\\\n",
    "\\\n",
    "\\\n",
    ".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b4881-ca58-4441-919f-c0471668a5ff",
   "metadata": {},
   "source": [
    "---\n",
    "2. b) For $θ_0^*$ and $θ_1^*$, plot the residuals vs the independent variable x. Print the mean and variance of the residuals. Briefly comment on what you observe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928b2e6f-b986-4301-acc0-f2ecc59a16f7",
   "metadata": {},
   "source": [
    "Residuals are the difference between the observed (y) and predicted responses (h). \n",
    "\n",
    "If the model is appropriate for the data, this should be reflected in the residuals. In particular, the graph of\n",
    "residuals vs. independent variable should be centered in zero without any tendency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74b4d7d-f3d2-4b52-9780-da9917070ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef1a552-e053-4d17-bd7c-cc2af8256885",
   "metadata": {},
   "source": [
    "The closer the residuals are from zero the smaller the cost J.\n",
    "\n",
    "For the normal error regression model, it is assumed that the error term is normally distributed with null mean and\n",
    "constant variance The mean and variance of the residuals are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974fa3e-7c2c-4e25-92e1-4f5a7411cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502b85e3-8fdb-454c-8ad0-a6afaa3825dd",
   "metadata": {},
   "source": [
    "---\n",
    "2. c) For $θ_0^*$ and $θ_1^*$, analytically prove that the sum of residuals is zero. Print the sum of the residuals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866b0a5b-2fab-4560-b4fd-3e462510d26f",
   "metadata": {},
   "source": [
    "- ie. show that: $\\qquad\\qquad\\qquad\\qquad \\sum_{i=1}^{m} \\left( h_θ(x^{(i)}) - y^{(i)} \\right) = 0 \\qquad\\qquad\\qquad for: \\qquad θ_0^*$,&emsp;$θ_1^*$\n",
    "\n",
    "- since:\n",
    "\n",
    "  $\\qquad\\qquad h_θ(x) = θ_0^* + θ_1^*x$\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    \\sum_{i=1}^{m} \\left( h_θ(x^{(i)}) - y^{(i)}  \\right) = \\sum_{i=1}^{m} \\left[ (θ_0^* + θ_1^*x)-y^{(i)} \\right]\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "- $θ_0^*$ is constant, so:\n",
    "$$\n",
    "\\begin{split}\n",
    "    &= mθ_0^*+ \\sum_{i=1}^{m}(θ_1^*x^{(i)}-y^{(i)}) \\\\\n",
    "    &= mθ_0^* + θ_1^* \\sum_{i=1}^{m} x^{(i)} - \\sum_{i=1}^{m} y^{(i)} \n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "$\\color{red}{\\text{To complete ...}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b24a277-94e5-481b-9389-d42a30a10767",
   "metadata": {},
   "source": [
    "---\n",
    "The Sum of the residuals is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2752706-30aa-4e52-ac0d-4e476cc48ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2768b-b151-42d6-8ab7-0919b511bd81",
   "metadata": {},
   "source": [
    "---\n",
    "2. d) Prove that the derivative of the cost function in order to $\\theta_j$ is:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\frac{\\partial}{\\partial \\theta_j} J(θ_0, \\dots, θ_n) = \n",
    " \\frac{1}{m} \\sum_{i=1}^{m} (h_\\theta(x_j^{(i)})-y^{(i)})   x_j^{(i)}\\quad\\text,\\quad j=0,1 \\dots n\n",
    "\\end{split}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "07e14ca2-af09-480d-a982-3a75165a30b5",
   "metadata": {},
   "source": [
    "The derivative of the cost function in order to $\\theta_j$: \n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "& \\frac{\\partial}{\\partial \\theta_j} \\quad J(θ_0, \\dots, θ_n) \\\\\n",
    "& \\frac{\\partial}{\\partial \\theta_j} \\quad \\frac{1}{2m}\\sum_{i=1}^{m} (h_θ(x_j^{(i)}) - y^{(i)})^2 \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "using the derivative of the product rule: \n",
    "\n",
    "$\\qquad (f.g)' = f'.g + f.g'$\n",
    "\n",
    "and assuming: \n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    "& \\quad f = \\frac{1}{2m} \\\\\n",
    "& \\quad g = \\sum_{i=1}^{m}(h_θ(x_j^{(i)}) - y^{(i)})^2\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "since it is a partial derivative in order to $\\theta_j, f$ is constant, $f'$ is zero:\n",
    "\n",
    "we have: $\\qquad 0.g + f.g'$\n",
    "\n",
    "now, using the derivative of the sum rule:\n",
    "\n",
    "$\n",
    "\\begin{split}\n",
    " \\qquad (f_0+f_1+\\dots+f_n)' &= f_0'+f_1'+\\dots+f_n' \\\\\n",
    " \\quad (\\sum_{i=1}^{m} f)' &= \\sum_{i=1}^{m} f'\n",
    "\\end{split}\n",
    "$\n",
    "\n",
    "we got:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2m} \\quad \\sum_{i=1}^{m} \\quad \\frac{\\partial}{\\partial \\theta_j} (h_θ(x_j^{(i)}) - y^{(i)})^2 \n",
    "$$\n",
    "\n",
    "using the derivative of the power rule:\n",
    "\n",
    "$\\qquad (g^n)' = n.g^{n-1}.g'$\n",
    "\n",
    "we got:\n",
    "\n",
    "$$\n",
    "\\frac{1}{\\cancel 2 m} \\quad \\sum_{i=1}^{m} \\quad \\cancel 2.(h_θ(x_j^{(i)}) - y^{(i)}) \\quad \\frac{\\partial}{\\partial \\theta_j} (h_θ(x_j^{(i)}) - y^{(i)}) \n",
    "$$\n",
    "\n",
    "\\\n",
    "$\\color{red}{\\text{To complete ...}}$\n",
    "\n",
    "\n",
    "\n",
    "List of derivative rules:\n",
    "\n",
    "https://www.mathsisfun.com/calculus/derivatives-rules.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a46d67-5e97-433d-b73b-14f8ad497491",
   "metadata": {},
   "source": [
    "---\n",
    "3. a) Express in vector notation the following gradient descent updating expressions: </p>\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \n",
    "\\sum_{i=1}^{m} (h_\\theta(x_j^{(i)})-y^{(i)})   x_j^{(i)}\\quad\\text,\\quad j=0,1 \\dots n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2de011-1e70-4270-907d-729f62199b36",
   "metadata": {},
   "source": [
    "using $\\mathbf{h}$, $\\mathbf{y}$, $\\boldsymbol{\\theta}$ and $\\mathbf{X}$ as defined in 2. a) ii):\n",
    "\n",
    "where it is also shown that:\n",
    "\n",
    "$$\\sum_{i=1}^{m} (h_\\theta(x_j^{(i)})-y^{(i)}) = \\mathbf{h}-\\mathbf{y}$$\n",
    "\n",
    "$\\color{red}{\\text{To complete ...}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d8021d-8f9c-4b9e-b8dd-eab648137c67",
   "metadata": {},
   "source": [
    "---\n",
    "3. b) Apply gradient descent and linear regression to the dataset. Find $\\theta_0*, \\theta_1*$ and $J(\\theta_0^*,\\theta_1^*)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c44c04-86b8-42ad-acd8-bf21bacfc0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c73a3d-988c-4171-85f0-5debd90500c5",
   "metadata": {},
   "source": [
    "---\n",
    "3. c) What is the number of iterations and the value of the learning rate $\\alpha$, that approximate the $\\theta$ vector and the cost J obtained in 3. b) to those found in 1. a) and 2. a)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392a2666-02e0-4515-8ddc-ba6562402c7a",
   "metadata": {},
   "source": [
    "$\\color{red}{\\text{Answer here ...}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7991f1cc-48e0-48fa-a3a2-7b9c7a77567b",
   "metadata": {},
   "source": [
    "---\n",
    "3. d)\tPlot $J(\\theta_0,\\theta_1)$ as a function of the number of iterations. Briefly comment on what you observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bb1b41-7eec-4eb6-b9ef-d0eb65f29c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
